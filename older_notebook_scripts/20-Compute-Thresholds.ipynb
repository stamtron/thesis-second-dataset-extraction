{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('./scratch')\n",
    "folder='train2'\n",
    "data_dir = path/'cvs'\n",
    "file_format = 'cv-test_seed-{}-train_seed-{}.xlsx'\n",
    "\n",
    "y_preds_per_validation = []\n",
    "y_target_per_validation = []\n",
    "\n",
    "\n",
    "test_seed = test_seeds[0]\n",
    "for train_val_seed in train_seeds:\n",
    "    data_file = data_dir / file_format.format(test_seed, train_val_seed)\n",
    "    df_train_val, _ = nsea_load_data_from_excel(data_file)\n",
    "    data=create_db_train_val(df_train_val, path, folder)\n",
    "    model_name = \"resnet152-full-model-test_seed-{}-train_seed-{}\".format(test_seed, train_val_seed)\n",
    "    y_preds_valid, _ = nsea_get_preds(model_name, data, None)\n",
    "    y_preds_per_validation.append(y_preds_valid[0])\n",
    "    y_target_per_validation.append(y_preds_valid[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nsea_load_learner(model_name,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine predictions and target to estimate optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = torch.cat(y_preds_per_validation) # Concat predictions\n",
    "y_target = torch.cat(y_target_per_validation) # Concat targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_valid = [y_preds, y_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = nsea_compute_thresholds(y_preds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('resnet152-optimal-thresholds.pkl', 'wb')\n",
    "pickle.dump(thresholds, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20,8)\n",
    "font = {'family' : 'DejaVu Sans', 'weight' : 'normal', 'size'  : 22}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = dict()\n",
    "recalls = dict()\n",
    "Thresholds = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_target.numpy()\n",
    "y_pred = y_preds.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    precisions[i], recalls[i], Thresholds[i] = precision_recall_curve(y_true[:, i], y_pred[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_id = []\n",
    "for i in range(5): \n",
    "    re = recalls[i]\n",
    "    pre = precisions[i]\n",
    "    dist = [ np.sqrt((1-re)**2 + (1-pre)**2) for re, pre in zip(re, pre) ]\n",
    "    opt_id.append(dist.index(min(dist)))\n",
    "    t = Thresholds[i]\n",
    "    opt_thres = t[opt_id[i]]\n",
    "    f1_score_opt = 2.0*re[opt_id[i]]*pre[opt_id[i]] / (re[opt_id[i]]+pre[opt_id[i]])\n",
    "    print(re[opt_id[i]], pre[opt_id[i]], f1_score_opt)\n",
    "    print(\"Optimal \",classes[i],\" Threshold = \", opt_thres )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmaps = OrderedDict()\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5): \n",
    "    plt.plot(recalls[i][3:-4], precisions[i][3:-4], label = classes[i], linewidth=3.0)\n",
    "    #plt.plot([recalls[i][opt_id[i]],1],[precisions[i][opt_id[i]],1], 'ro--')\n",
    "    plt.scatter(recalls[i][opt_id[i]],precisions[i][opt_id[i]], marker='*', s=900)\n",
    "#     plt.text(0.85, 0.8, 'opt. threshold', color='r', size=18)\n",
    "plt.xlabel('Recall', size=20)\n",
    "plt.ylabel('Precision', size=20)\n",
    "plt.xlim(0.75,1)\n",
    "plt.ylim(0.75,1)\n",
    "plt.title(\"Precision Recall Curves\")\n",
    "#plt.legend()\n",
    "#plt.axis('off')\n",
    "#plt.axis('equal')\n",
    "#plt.gca().set_aspect('equal')\n",
    "plt.savefig('figures/precision_recall_curve_zoom.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds = pd.DataFrame({'Threshold': thresholds}).T\n",
    "df_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_thresholds.to_latex(float_format=lambda x: '%.3f' % truncate_decimals(x,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get performance on each fold using the thresholds computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_compute_metrics(y_target_per_validation[0], y_preds_per_validation[0], thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "for fold_idx,fold in enumerate(zip(y_preds_per_validation, y_target_per_validation)):\n",
    "    print('Compute Performance on fold #', fold_idx)\n",
    "    result = new_compute_metrics(fold[1], fold[0], thresholds)\n",
    "    #write_dictionary_to_csv('results/resnet34-validation-metrics-{}-{}.csv'.format(test_seed,train_seeds[fold_idx]), res)\n",
    "    print(test_seed, train_seeds[fold_idx])\n",
    "    display(result)\n",
    "    res.append(result)\n",
    "    print('='*60)\n",
    "    #print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summary=df.groupby('Event').agg({\n",
    "    'Threshold': 'mean',\n",
    "    'Exact Matching Score': ['mean', 'std'], \n",
    "    'Recall': ['mean', 'std'], \n",
    "    'Precision': ['mean', 'std'], \n",
    "    'F1-Score': ['mean', 'std']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_summary.to_latex(float_format=lambda x: '%.3f' % truncate_decimals(x,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_folds=df.query('Event==\"Aggregate\"')\n",
    "df_folds.reset_index(drop=True, inplace=True)\n",
    "df_folds.index = df_folds.index+1\n",
    "df_folds.index.name='Fold #'\n",
    "df_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_folds[['Exact Matching Score', 'Precision', 'Recall', 'F1-Score']].to_latex(float_format=lambda x: '%.3f' % truncate_decimals(x,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
