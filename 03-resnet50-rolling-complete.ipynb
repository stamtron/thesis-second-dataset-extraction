{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./helpers_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./data_visualization_and_augmentations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../torch_videovision/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./important_csvs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,10)\n",
    "font = {'family' : 'DejaVu Sans',  'weight' : 'normal',  'size'  : 20}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model, change head, freeze body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet50(pretrained=True)\n",
    "adaptive_pooling = AdaptiveConcatPool2d()\n",
    "head = Head()\n",
    "resnet.avgpool = adaptive_pooling\n",
    "resnet.fc = head\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = nn.DataParallel(resnet)\n",
    "check_freeze(resnet.module)\n",
    "\n",
    "#summary(resnet.module, torch.zeros(2,3,576,704).cuda())\n",
    "\n",
    "tensor_transform = get_tensor_transform('ImageNet', True)\n",
    "train_spat_transform = get_spatial_transform(2)\n",
    "train_temp_transform = get_temporal_transform()\n",
    "valid_spat_transform = get_spatial_transform(0)\n",
    "valid_temp_transform = va.TemporalFit(size=16)\n",
    "\n",
    "root_dir = '/media/scratch/astamoulakatos/nsea_video_jpegs/'\n",
    "df = pd.read_csv('./small_dataset_csvs/events_with_number_of_frames_stratified.csv')\n",
    "df_train = get_df(df, 20, True, False, False)\n",
    "class_image_paths, end_idx = get_indices(df_train, root_dir)\n",
    "train_loader = get_loader(1, 270, end_idx, class_image_paths, train_temp_transform, train_spat_transform, tensor_transform, False, True)\n",
    "df_valid = get_df(df, 20, False, True, False)\n",
    "class_image_paths, end_idx = get_indices(df_valid, root_dir)\n",
    "valid_loader = get_loader(1, 270, end_idx, class_image_paths, valid_temp_transform, valid_spat_transform, tensor_transform, False, True)\n",
    "df_test = get_df(df, 20, False, False, True)\n",
    "class_image_paths, end_idx = get_indices(df_test, root_dir)\n",
    "test_loader = get_loader(1, 270, end_idx, class_image_paths, valid_temp_transform, valid_spat_transform, tensor_transform, False, True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "lr = 1e-2\n",
    "epochs = 10\n",
    "optimizer = optim.AdamW(resnet.parameters(), lr=lr, weight_decay=1e-2)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"validation\": valid_loader\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(resnet, torch.zeros(2,3,576,704).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=lr, weight_decay=1e-2)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=6)\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"validation\": valid_loader\n",
    "}\n",
    "save_model_path = '/media/raid/astamoulakatos/saved-resnet-models/'\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate = scheduler.get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_yo(dataloaders, device, model, criterion, optimizer, scheduler, num_epochs=6):\n",
    "    #liveloss = PlotLosses()\n",
    "    model = model.to(device)\n",
    "    val_loss = 100\n",
    "    \n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    val_f1 = []\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    train_f1 = []\n",
    "    for epoch in range(num_epochs):\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0  \n",
    "            running_f1 = 0.0\n",
    "            #train_result = []\n",
    "            for counter, (inputs, labels) in enumerate(Bar(dataloaders[phase])):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                preds = torch.sigmoid(outputs).data > 0.5\n",
    "                preds = preds.to(torch.float32) \n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_acc += accuracy_score(labels.detach().cpu().numpy(), preds.cpu().detach().numpy()) *  inputs.size(0)\n",
    "                running_f1 += f1_score(labels.detach().cpu().numpy(), (preds.detach().cpu().numpy()), average=\"samples\")  *  inputs.size(0)\n",
    "           \n",
    "                if (counter!=0) and (counter%100==0):\n",
    "                    if phase == 'train':\n",
    "                        result = '  Training Loss: {:.4f} Acc: {:.4f} F1: {:.4f}'.format(running_loss/(inputs.size(0)*counter),\n",
    "                                                                                         running_acc/(inputs.size(0)*counter),\n",
    "                                                                                         running_f1/(inputs.size(0)*counter))\n",
    "                        print(result)\n",
    "                    if phase == 'validation':\n",
    "                        result = '  Validation Loss: {:.4f} Acc: {:.4f} F1: {:.4f}'.format(running_loss/(inputs.size(0)*counter),\n",
    "                                                                                         running_acc/(inputs.size(0)*counter),\n",
    "                                                                                         running_f1/(inputs.size(0)*counter))\n",
    "                        print(result)\n",
    "                        \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "            epoch_f1 = running_f1 / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "                train_f1.append(epoch_f1)\n",
    "            \n",
    "            #prefix = ''\n",
    "            if phase == 'validation':\n",
    "                #prefix = 'val_'\n",
    "                val_losses.append(epoch_loss)\n",
    "                val_acc.append(epoch_acc)\n",
    "                val_f1.append(epoch_f1)\n",
    "                \n",
    "                if epoch_loss < val_loss:\n",
    "                    val_loss = epoch_loss\n",
    "                    save_path = f'{save_model_path}/best-checkpoint-{str(epoch).zfill(3)}epoch.pth'\n",
    "                    states = {  'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                                'val_loss': epoch_loss,\n",
    "                                'epoch': epoch,  }\n",
    "                    \n",
    "                    torch.save(states, save_file_path)\n",
    "                    for path in sorted(glob(f'{save_model_path}/best-checkpoint-*epoch.pth'))[:-3]:\n",
    "                        os.remove(path)\n",
    "                \n",
    "#             logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "#             logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "#             logs[prefix + 'f1_score'] = epoch_f1.item()\n",
    "            \n",
    "#         liveloss.update(logs)\n",
    "#         liveloss.send()\n",
    "        with open(\"resnet_val_losses.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(val_losses, fp)\n",
    "        with open(\"resnet_val_acc.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(val_acc, fp)\n",
    "        with open(\"resnet_val_f1.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(val_f1, fp)\n",
    "        with open(\"resnet_train_losses.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(train_losses, fp)\n",
    "        with open(\"resnet_train_acc.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(train_acc, fp)\n",
    "        with open(\"resnet_train_f1.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(train_f1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_yo(dataloaders, device, resnet, criterion, optimizer, scheduler, num_epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
