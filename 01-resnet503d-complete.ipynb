{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./helpers_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./data_visualization_and_augmentations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../torch_videovision/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../3D-ResNets-PyTorch/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./important_csvs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_3d import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,10)\n",
    "font = {'family' : 'DejaVu Sans',  'weight' : 'normal',  'size'  : 20}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model, change head, freeze body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"model_depth\": 50,\n",
    "    \"model\": 'resnet',\n",
    "    \"n_classes\": 400,\n",
    "    \"n_finetune_classes\": 5,\n",
    "    \"resnet_shortcut\": 'B',\n",
    "    \"sample_size\": (576,704),\n",
    "    \"sample_duration\": 16,\n",
    "    \"pretrain_path\": '../3D-ResNets-PyTorch/resnet-50-kinetics.pth',\n",
    "    \"no_cuda\": False,\n",
    "    \"arch\": 'resnet-50',\n",
    "    \"ft_begin_index\": 0\n",
    "}\n",
    "\n",
    "opts = namedtuple(\"opts\", sorted(options.keys()))\n",
    "\n",
    "myopts = opts(**options)\n",
    "\n",
    "model, parameters = generate_model(myopts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_pooling = AdaptiveConcatPool3d()\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "#torch.cuda.empty_cache()\n",
    "device = torch.device('cuda') \n",
    "head = Head()\n",
    "adaptive_pooling = adaptive_pooling.to(device)\n",
    "head = head.to(device)\n",
    "model.module.avgpool = adaptive_pooling\n",
    "model.module.fc = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model.module.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in model.module.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(model.module, (3,16,576,704))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_transform = get_tensor_transform('Kinetics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = get_video_transform(2)\n",
    "valid_transform = get_video_transform(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./important_csvs/events_with_number_of_frames_stratified.csv')\n",
    "df = get_df(df, 16, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_image_paths, end_idx = get_indices(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loader(16, 4, end_idx, class_image_paths, train_transform, tensor_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_batch(train_loader,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./important_csvs/events_with_number_of_frames_stratified.csv')\n",
    "df = get_df(df, 16, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_image_paths, end_idx = get_indices(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = get_loader(16, 4, end_idx, class_image_paths, valid_transform, tensor_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_batch(valid_loader, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "# lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "# lr_finder.range_test(train_loader, end_lr=100, num_iter=200)\n",
    "# lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "# lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 6e-2; lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop with live losses plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"validation\": valid_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on cuda if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '/media/raid/astamoulakatos/saved-3d-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=6):\n",
    "    #liveloss = PlotLosses()\n",
    "    model = model.to(device)\n",
    "    val_loss = 100\n",
    "    \n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    val_f1 = []\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    train_f1 = []\n",
    "    for epoch in range(num_epochs):\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0  \n",
    "            running_f1 = 0.0\n",
    "            #train_result = []\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                preds = torch.sigmoid(outputs).data > 0.5\n",
    "                preds = preds.to(torch.float32) \n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_acc += accuracy_score(labels.detach().cpu().numpy(), preds.cpu().detach().numpy()) *  inputs.size(0)\n",
    "                running_f1 += f1_score(labels.detach().cpu().numpy(), (preds.detach().cpu().numpy()), average=\"samples\")  *  inputs.size(0)\n",
    "           \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "            epoch_f1 = running_f1 / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "                train_f1.append(epoch_f1)\n",
    "            \n",
    "            #prefix = ''\n",
    "            if phase == 'validation':\n",
    "                #prefix = 'val_'\n",
    "                val_losses.append(epoch_loss)\n",
    "                val_acc.append(epoch_acc)\n",
    "                val_f1.append(epoch_f1)\n",
    "                \n",
    "                if epoch_loss < val_loss:\n",
    "                    val_loss = epoch_loss\n",
    "                    save_path = f'{save_model_path}/best-checkpoint-{str(epoch).zfill(3)}epoch.pth'\n",
    "                    states = {  'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                                'val_loss': epoch_loss,\n",
    "                                'epoch': epoch,  }\n",
    "                    \n",
    "                    torch.save(states, save_file_path)\n",
    "                    for path in sorted(glob(f'{save_model_path}/best-checkpoint-*epoch.pth'))[:-3]:\n",
    "                        os.remove(path)\n",
    "                \n",
    "#             logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "#             logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "#             logs[prefix + 'f1_score'] = epoch_f1.item()\n",
    "            \n",
    "#         liveloss.update(logs)\n",
    "#         liveloss.send()\n",
    "        with open(\"val_losses.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(val_losses, fp)\n",
    "        with open(\"val_acc.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(val_acc, fp)\n",
    "        with open(\"val_f1.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(val_f1, fp)\n",
    "        with open(\"train_losses.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(train_losses, fp)\n",
    "        with open(\"train_acc.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(train_acc, fp)\n",
    "        with open(\"train_f1.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(train_f1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, criterion, optimizer, scheduler, num_epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved model, unfreeze body, train for more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "l = [1,2,3,4]\n",
    "with open(\"test.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(l, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    l.append(i)\n",
    "    with open(\"test.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(l, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
