{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./helpers_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./data_visualization_and_augmentations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../torch_videovision/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../video-classification/ResNetCRNN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./important_csvs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_lstm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,10)\n",
    "font = {'family' : 'DejaVu Sans',  'weight' : 'normal',  'size'  : 20}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_transform = get_tensor_transform('ImageNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = get_video_transform(2)\n",
    "valid_transform = get_video_transform(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./important_csvs/events_with_number_of_frames_stratified.csv')\n",
    "df = get_df(df, 16, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_image_paths, end_idx = get_indices(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loader(16, 4, end_idx, class_image_paths, train_transform, tensor_transform, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_batch(train_loader,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./important_csvs/events_with_number_of_frames_stratified.csv')\n",
    "df = get_df(df, 16, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_image_paths, end_idx = get_indices(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = get_loader(16, 4, end_idx, class_image_paths, valid_transform, tensor_transform, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model, change head, freeze body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder = ResCNNEncoder().to(device)\n",
    "adaptive_pool = AdaptiveConcatPool2d()\n",
    "cnn_encoder.resnet[8] = adaptive_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in cnn_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in cnn_encoder.resnet[8].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in cnn_encoder.headbn1.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in cnn_encoder.fc1.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_decoder = DecoderRNNattention(batch_size=4).to(device)\n",
    "for param in rnn_decoder.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn_params, cnn_encoder, rnn_decoder = parallelize_model(cnn_encoder, rnn_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(cnn_encoder.module,rnn_decoder.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, torch.zeros(2,16,3,576,704).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cnn_encoder.module, torch.zeros(2,16,3,576,704).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_decoder.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(rnn_decoder.module, torch.zeros(2,16,512).cuda()) #,  change the division by 2 to wok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(cnn_encoder,rnn_decoder) #!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(crnn_params, lr=1e-7, weight_decay=1e-2)\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(train_loader, end_lr=100, num_iter=200)\n",
    "lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1; lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(crnn_params, lr=lr, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"validation\": valid_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '/media/raid/astamoulakatos/saved-lstm-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=6):\n",
    "    #liveloss = PlotLosses()\n",
    "    model = model.to(device)\n",
    "    val_loss = 100\n",
    "    \n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    val_f1 = []\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    train_f1 = []\n",
    "    for epoch in range(num_epochs):\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0  \n",
    "            running_f1 = 0.0\n",
    "            #train_result = []\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                preds = torch.sigmoid(outputs).data > 0.5\n",
    "                preds = preds.to(torch.float32) \n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_acc += accuracy_score(labels.detach().cpu().numpy(), preds.cpu().detach().numpy()) *  inputs.size(0)\n",
    "                running_f1 += f1_score(labels.detach().cpu().numpy(), (preds.detach().cpu().numpy()), average=\"samples\")  *  inputs.size(0)\n",
    "           \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "            epoch_f1 = running_f1 / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "                train_f1.append(epoch_f1)\n",
    "            \n",
    "            #prefix = ''\n",
    "            if phase == 'validation':\n",
    "                #prefix = 'val_'\n",
    "                val_losses.append(epoch_loss)\n",
    "                val_acc.append(epoch_acc)\n",
    "                val_f1.append(epoch_f1)\n",
    "                \n",
    "                if epoch_loss < val_loss:\n",
    "                    val_loss = epoch_loss\n",
    "                    save_path = f'{save_model_path}/best-checkpoint-{str(epoch).zfill(3)}epoch.pth'\n",
    "                    states = {  'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                                'val_loss': epoch_loss,\n",
    "                                'epoch': epoch,  }\n",
    "                    \n",
    "                    torch.save(states, save_file_path)\n",
    "                    for path in sorted(glob(f'{save_model_path}/best-checkpoint-*epoch.pth'))[:-3]:\n",
    "                        os.remove(path)\n",
    "                \n",
    "#             logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "#             logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "#             logs[prefix + 'f1_score'] = epoch_f1.item()\n",
    "            \n",
    "#         liveloss.update(logs)\n",
    "#         liveloss.send()\n",
    "        with open(\"cnnlstm_val_losses.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(val_losses, fp)\n",
    "        with open(\"cnnlstm_val_acc.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(val_acc, fp)\n",
    "        with open(\"cnnlstm_val_f1.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(val_f1, fp)\n",
    "        with open(\"cnnlstm_train_losses.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(train_losses, fp)\n",
    "        with open(\"cnnlstm_train_acc.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(train_acc, fp)\n",
    "        with open(\"cnnlstm_train_f1.txt\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(train_f1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, criterion, optimizer, scheduler, num_epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved model for more training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
