{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../video-classification/ResNetCRNN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from functions_new import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from torchsummary1 import summary\n",
    "\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/media/hdd/astamoulakatos/nsea_video_jpegs/'\n",
    "class_paths = [d.path for d in os.scandir(root_dir) if d.is_dir]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((576, 704)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class_names = ['exp_and','exp_fs','exp','exp_fj','bur']\n",
    "one_hot_classes = [[1,0,1,0,0],[1,0,0,0,1],[1,0,0,0,0],[1,0,0,1,0],[0,1,0,0,0]]\n",
    "\n",
    "df = pd.read_csv('./train-valid-splits-video/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_image_paths = []\n",
    "end_idx = []\n",
    "for c, class_path in enumerate(class_paths):\n",
    "    for d in os.scandir(class_path):\n",
    "        if d.is_dir:\n",
    "            if d.path in df.videos.values:\n",
    "                paths = sorted(glob.glob(os.path.join(d.path, '*.png')))\n",
    "                # Add class idx to paths\n",
    "                paths = [(p, one_hot_classes[c]) for p in paths]\n",
    "                class_image_paths.extend(paths)\n",
    "                end_idx.extend([len(paths)])\n",
    "                \n",
    "end_idx = [0, *end_idx]\n",
    "end_idx = torch.cumsum(torch.tensor(end_idx), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = MySampler(end_idx, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(\n",
    "    image_paths=class_image_paths,\n",
    "    seq_length=seq_length,\n",
    "    transform=transform,\n",
    "    length=len(sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=bs,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get a batch of training data\n",
    "# inputs, classes = next(iter(loader))\n",
    "# inputs = inputs.squeeze(dim = 0)\n",
    "\n",
    "# for j in range(bs):\n",
    "#     # Make a grid from batch\n",
    "#     out = torchvision.utils.make_grid(inputs[j])\n",
    "\n",
    "\n",
    "#     for i, f in enumerate(one_hot_classes):\n",
    "#         if np.array_equal(classes[j][0].numpy(), np.asarray(f)):\n",
    "#             title = class_names[i]\n",
    "\n",
    "\n",
    "#     imshow(out, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"./save-model-lstm/\"\n",
    "\n",
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n",
    "res_size = (576, 704)      # ResNet image size\n",
    "dropout_p = 0.4      # dropout probability\n",
    "\n",
    "# DecoderRNN architecture\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 256\n",
    "\n",
    "# training parameters\n",
    "k = 5            # number of target category\n",
    "epochs = 2        # training epochs\n",
    "batch_size = bs\n",
    "learning_rate = 1e-3\n",
    "log_interval = 10   # interval for displaying training info\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "#begin_frame, end_frame, skip_frame = 1, 29, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2,\n",
    "                            drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cnn_encoder, (16,3,576,704))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(rnn_decoder, (16,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize model to multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    cnn_encoder = nn.DataParallel(cnn_encoder)\n",
    "    rnn_decoder = nn.DataParallel(rnn_decoder)\n",
    "\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.module.fc1.parameters()) + list(cnn_encoder.module.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.module.fc2.parameters()) + list(cnn_encoder.module.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.module.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "\n",
    "elif torch.cuda.device_count() == 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(crnn_params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    #cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device) #.view(-1, )\n",
    " \n",
    "        y = y.squeeze(dim=1)\n",
    "\n",
    "        #y = y.type_as(output) # comment that line the first time and uncomment it after that\n",
    "\n",
    "        y = y.float()\n",
    "        N_count += X.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        #y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        #step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        #scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(X), len(loader.dataset),\n",
    "                    100. * batch_idx / len(loader), loss.item())) #data[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_acc(original, predicted):\n",
    "    return torch.round(predicted).eq(original).sum().numpy()/len(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Pytorch models of best record\n",
    "torch.save(cnn_encoder.state_dict(),\n",
    "           os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "torch.save(rnn_decoder.state_dict(),\n",
    "           os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "torch.save(optimizer.state_dict(),\n",
    "           os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))  # save optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/media/scratch/astamoulakatos/nsea_video_jpegs/'\n",
    "class_paths = [d.path for d in os.scandir(root_dir) if d.is_dir]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((576, 704)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class_names = ['exp_and','exp_fs','exp','exp_fj','bur']\n",
    "one_hot_classes = [[1,0,1,0,0],[1,0,0,0,1],[1,0,0,0,0],[1,0,0,1,0],[0,1,0,0,0]]\n",
    "\n",
    "df = pd.read_csv('./train-valid-splits-video/valid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_image_paths = []\n",
    "end_idx = []\n",
    "for c, class_path in enumerate(class_paths):\n",
    "    for d in os.scandir(class_path):\n",
    "        if d.is_dir:\n",
    "            if d.path in df.videos.values:\n",
    "                paths = sorted(glob.glob(os.path.join(d.path, '*.png')))\n",
    "                # Add class idx to paths\n",
    "                paths = [(p, one_hot_classes[c]) for p in paths]\n",
    "                class_image_paths.extend(paths)\n",
    "                end_idx.extend([len(paths)])\n",
    "                \n",
    "end_idx = [0, *end_idx]\n",
    "end_idx = torch.cumsum(torch.tensor(end_idx), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = MySampler(end_idx, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(\n",
    "    image_paths=class_image_paths,\n",
    "    seq_length=seq_length,\n",
    "    transform=transform,\n",
    "    length=len(sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=bs,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder.eval()\n",
    "rnn_decoder.eval()\n",
    "\n",
    "test_loss = 0\n",
    "all_y = []\n",
    "all_y_pred = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in valid_loader:\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y = y.squeeze(dim=1)\n",
    "        \n",
    "        y = y.float()\n",
    "\n",
    "        #y = y.type_as(output) # comment that line the first time and uncomment it after that\n",
    "\n",
    "        output = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(output, y)\n",
    "        test_loss += loss.item()   \n",
    "        # sum up batch loss\n",
    "        y_pred = output.sigmoid()\n",
    "#         if sigmoid: y_pred = y_pred.sigmoid()\n",
    "#         return ((y_pred>thresh).byte()==y_true.byte()).float().mean()\n",
    "\n",
    "        # collect all y and y_pred in all batches\n",
    "        all_y.extend(y)\n",
    "        all_y_pred.extend(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss /= len(valid_loader.dataset)\n",
    "\n",
    "# compute accuracy\n",
    "#all_y = torch.stack(all_y, dim=0)\n",
    "#all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "#test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "accuracy = ((all_y_pred>0.5).byte() == all_y.byte()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_encoder, rnn_decoder = model\n",
    "cnn_encoder.eval()\n",
    "rnn_decoder.eval()\n",
    "\n",
    "cl1 = []\n",
    "cl2 = []\n",
    "cl3 = []\n",
    "cl4 = []\n",
    "cl5 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in loader:\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device) #.view(-1, )\n",
    "        y = y.squeeze(dim=1)\n",
    "\n",
    "        y = y.type_as(output)\n",
    "\n",
    "        output = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "        pred = torch.sigmoid(output)\n",
    "        pred = pred.cpu().numpy()\n",
    "        for p in pred:\n",
    "            cl1.append(p[0])\n",
    "            cl2.append(p[1])\n",
    "            cl3.append(p[2])\n",
    "            cl4.append(p[3])\n",
    "            cl5.append(p[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'anode':cl1, 'burial':cl2, 'exposure':cl3, 'field joint':cl4, 'free span':cl5} \n",
    "  \n",
    "df_pred = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
